{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for the training of the Teresa Robot\n",
    "## Importing all necessary tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From /home/danielrs/Work/Summer_Intership/venv/lib/python3.6/site-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "2021-02-04 17:59:53,616  WARNING: From /home/danielrs/Work/Summer_Intership/venv/lib/python3.6/site-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "from src.gym_envs.RobotEnv import RobotEnv # Training environment\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "import roslibpy # API of ROS\n",
    "from src.robots.Teresa import Teresa # This is the representation of Teresa Robot\n",
    "from src.utils.training_tools import NB_STATES\n",
    "\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the connection with ROS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HOST = 'localhost'\n",
    "PORT = 9090\n",
    "\n",
    "client = roslibpy.Ros(host=HOST, port=PORT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary functions for the training (Neural Network Set up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-ed69ceeb7041>:53: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "2020-11-27 15:04:00,924  WARNING: From <ipython-input-3-ed69ceeb7041>:53: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /home/danielrs/Work/Summer_Intership/venv/lib/python3.6/site-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "2020-11-27 15:04:00,943  WARNING: From /home/danielrs/Work/Summer_Intership/venv/lib/python3.6/site-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "state_size =  NB_STATES # 800 is the image size this maybe variable\n",
    "action_size = 4\n",
    "new_graph = tf.Graph()\n",
    "## TRAINING Hyperparameters\n",
    "\n",
    "initializer=tf.initializers.glorot_uniform()\n",
    "\n",
    "def discount_correct_rewards(r, gamma=0.99):\n",
    "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "  discounted_r = np.zeros_like(r)\n",
    "  running_add = 0\n",
    "  for t in reversed(range(0, r.size)):\n",
    "    #if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
    "    running_add = running_add * gamma + r[t]\n",
    "    discounted_r[t] = running_add\n",
    "\n",
    "  discounted_r -= discounted_r.mean()\n",
    "  discounted_r /- discounted_r.std()\n",
    "  return discounted_r\n",
    "\n",
    "def discount_and_normalize_rewards(episode_rewards):\n",
    "    discounted_episode_rewards = np.zeros_like(episode_rewards)\n",
    "    cumulative = 0.0\n",
    "    #print(\"len episode rewards\",episode_rewards)\n",
    "    for i in reversed(range(len(episode_rewards))):\n",
    "        cumulative = cumulative * gamma + episode_rewards[i]\n",
    "        #print(\"dans boucle\",episode_rewards[i],\"cyl\",cumulative)\n",
    "        discounted_episode_rewards[i] = cumulative\n",
    "    \n",
    "    mean = np.mean(discounted_episode_rewards)\n",
    "    std = np.std(discounted_episode_rewards)\n",
    "    if std :\n",
    "        discounted_episode_rewards = (discounted_episode_rewards - mean) / (std)\n",
    "    else:\n",
    "        discounted_episode=[]\n",
    "        discounted_episode_rewards[0] = np.array(mean)\n",
    "        print(\"ATTTTTTTTTTTTTTTTTT\")\n",
    "    #print(\"dis\",discounted_episode_rewards,\"std\",std)\n",
    "    \n",
    "    return discounted_episode_rewards\n",
    "\n",
    "with tf.name_scope(\"inputs\"):\n",
    "    input_ = tf.placeholder(tf.float32, [None, state_size], name=\"input_\")\n",
    "    actions = tf.placeholder(tf.int32, [None, action_size], name=\"actions\")\n",
    "    discounted_episode_rewards_ = tf.placeholder(tf.float32, [None,], name=\"discounted_episode_rewards\")\n",
    "    \n",
    "    # Add this placeholder for having this variable in tensorboard\n",
    "    mean_reward_ = tf.placeholder(tf.float32 , name=\"mean_reward\")\n",
    "\n",
    "    with tf.name_scope(\"fc1\"):\n",
    "        fc1 = tf.layers.dense(input_ , 20, activation=tf.nn.relu,kernel_initializer=initializer)\n",
    "\n",
    "    with tf.name_scope(\"fc2\"):\n",
    "        fc2 = tf.layers.dense(fc1, action_size,activation= tf.nn.relu, kernel_initializer=initializer)\n",
    "    \n",
    "    with tf.name_scope(\"fc3\"):\n",
    "        fc3 = tf.layers.dense(fc2, action_size, activation= None,kernel_initializer=initializer)\n",
    "\n",
    "    with tf.name_scope(\"softmax\"):\n",
    "        action_distribution = tf.nn.softmax(fc3)\n",
    "\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        # tf.nn.softmax_cross_entropy_with_logits computes the cross entropy of the result after applying the softmax function\n",
    "        # If you have single-class labels, where an object can only belong to one class, you might now consider using \n",
    "        # tf.nn.sparse_softmax_cross_entropy_with_logits so that you don't have to convert your labels to a dense one-hot array. \n",
    "        neg_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(logits = fc3, labels = actions)\n",
    "        #loss = tf.nn.sparse_softmax_cross_entropy_with_logits (neg_log_prob * discounted_episode_rewards_)\n",
    "        loss = tf.reduce_mean(neg_log_prob * discounted_episode_rewards_) \n",
    "        \n",
    "    \n",
    "    with tf.name_scope(\"train\"):\n",
    "        train_opt = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "# Setup TensorBoard Writer\n",
    "\n",
    "## Losses\n",
    "tf.summary.scalar(\"Loss\", loss)\n",
    "\n",
    "## Reward mean\n",
    "tf.summary.scalar(\"Reward_mean\", mean_reward_)\n",
    "\n",
    "merged_summary_op = tf.summary.merge_all() #procedure d'affichage group√©e dans tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating representation of the robot and introducing it in the Gym Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# client.connect()\n",
    "client.run() # This run the main loop of ROS\n",
    "teresa_controller = Teresa(client) # Robot API\n",
    "env = RobotEnv(teresa_controller, client) # Training Environment\n",
    "\n",
    "env.reset() # Restarting the environment to the initial state\n",
    "import time\n",
    "for i in range(20):\n",
    "    state, reward, done, _ = env.step(np.random.randint(4))\n",
    "    # print(state)\n",
    "    env.render()\n",
    "    if done and reward:\n",
    "        print(\"Body detected and centered\")\n",
    "        env.reset()\n",
    "    elif done:\n",
    "        print(\"Face not detected. End of the episode\")\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controlling the Robot Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2020-11-27 15:01:50,462     INFO: Connection to ROS MASTER ready.\n",
      "Centered\n",
      "Centered\n",
      "Centered\n",
      "Centered\n"
     ]
    }
   ],
   "source": [
    "client.run()\n",
    "teresa_controller = Teresa(client)\n",
    "env = RobotEnv(teresa_controller, client)\n",
    "\n",
    "env.reset()\n",
    "finish = False\n",
    "\n",
    "while not finish:\n",
    "    movement = input('Enter a movement (0 Right, 1 Left, 2 Backward, 3 Forward, exit): ')\n",
    "    if movement == 'exit':\n",
    "        finish = True\n",
    "        continue\n",
    "    movement = int(movement)\n",
    "    state, reward, done, _ = env.step(movement)\n",
    "    if done and reward:\n",
    "        print(\"Centered\")\n",
    "        # env.reset()\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "23268999]] action proba 0.25 length 4 rew 1\n",
      "==========================================\n",
      "Episode:  673 length 4\n",
      "Reward:  0.25\n",
      "Mean Reward val 0.35047276119487214\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.1290527  0.2975791  0.33873296 0.2346352 ]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  675 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3501755045049465\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.1301124  0.29774126 0.33584496 0.23630138]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  676 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.3501506268518126\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.12980694 0.2975561  0.33507928 0.23755766]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  678 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.34985563236918577\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.12955543 0.29762876 0.33421266 0.23860323]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  679 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3508117270274664\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.12862387 0.29697293 0.3353713  0.23903187]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  680 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.35176501377191943\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.12777425 0.29756773 0.33560017 0.23905782]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  681 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.35271550495407206\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.12631114 0.29737788 0.3377536  0.23855746]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  686 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.35160403839690996\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.12493479 0.2971323  0.33987844 0.23805438]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  687 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3518197302015656\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.1246849  0.29628858 0.34126744 0.23775908]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  688 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.35276048531012644\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.13297458 0.2973425  0.33129057 0.23839228]] action proba 0.14285714285714285 length 7 rew 1\n",
      "==========================================\n",
      "Episode:  689 length 7\n",
      "Reward:  0.14285714285714285\n",
      "Mean Reward val 0.352456277567441\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.14506543 0.29899037 0.31679145 0.23915282]] action proba 0.2 length 5 rew 1\n",
      "==========================================\n",
      "Episode:  690 length 5\n",
      "Reward:  0.2\n",
      "Mean Reward val 0.3522356461961422\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.12197014 0.291525   0.35113755 0.23536722]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  691 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.352449178499327\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.11987156 0.28960335 0.3560286  0.23449652]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  693 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.3519137822116248\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.11784962 0.2876521  0.36094272 0.23355551]] action proba 0.16666666666666666 length 6 rew 1\n",
      "==========================================\n",
      "Episode:  694 length 6\n",
      "Reward:  0.16666666666666666\n",
      "Mean Reward val 0.3516472395993299\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.11585847 0.2856255  0.36616778 0.23234826]] action proba 0.125 length 8 rew 1\n",
      "==========================================\n",
      "Episode:  695 length 8\n",
      "Reward:  0.125\n",
      "Mean Reward val 0.3513215970136987\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.11368757 0.2834778  0.37190557 0.23092906]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  696 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3522522690409387\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.11097029 0.28052247 0.3796099  0.22889733]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  698 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.35267500932980583\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.10856397 0.2780233  0.38648352 0.22692916]] action proba 0.2 length 5 rew 1\n",
      "==========================================\n",
      "Episode:  699 length 5\n",
      "Reward:  0.2\n",
      "Mean Reward val 0.3524569021736204\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.10635494 0.2757502  0.39219078 0.22570407]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  700 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.35338064411060527\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.10357399 0.27255598 0.40011874 0.2237513 ]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  701 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.3533520866878456\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.12781557 0.2830662  0.35840675 0.2307115 ]] action proba 0.25 length 4 rew 1\n",
      "==========================================\n",
      "Episode:  703 length 4\n",
      "Reward:  0.25\n",
      "Mean Reward val 0.35270335916884604\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.09913404 0.26703703 0.41261268 0.22121628]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  704 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.35362151043243634\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.09637883 0.26330152 0.42123345 0.21908616]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  705 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.35453706070094565\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.1111235  0.27047578 0.39275265 0.22564806]] action proba 0.25 length 4 rew 1\n",
      "==========================================\n",
      "Episode:  706 length 4\n",
      "Reward:  0.25\n",
      "Mean Reward val 0.3543892006433771\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.10817516 0.2675666  0.39912438 0.2251339 ]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  708 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3540947318122251\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.09047375 0.25454295 0.43668228 0.21830104]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  711 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.35400725400964556\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.08882438 0.25229955 0.44104472 0.21783134]] action proba 0.25 length 4 rew 1\n",
      "==========================================\n",
      "Episode:  712 length 4\n",
      "Reward:  0.25\n",
      "Mean Reward val 0.3538613812831243\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.08732569 0.24989429 0.44506457 0.21771544]] action proba 0.0 length 19 rew 0\n",
      "==========================================\n",
      "Episode:  713 length 19\n",
      "Reward:  0.0\n",
      "Mean Reward val 0.3533657771076577\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.0859782  0.24769966 0.44872198 0.21760018]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  714 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3542701606361785\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.08392757 0.2442638  0.45542032 0.21638834]] action proba 0.14285714285714285 length 7 rew 1\n",
      "==========================================\n",
      "Episode:  716 length 7\n",
      "Reward:  0.14285714285714285\n",
      "Mean Reward val 0.35348120222834695\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.08192852 0.24081267 0.46226004 0.2149988 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  717 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.35438164623638546\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.07929847 0.23613895 0.47203562 0.212527  ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  720 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.35429406657104684\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.07698632 0.2318098  0.480726   0.21047787]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  722 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3546971258613067\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.07477211 0.22751659 0.48937684 0.20833446]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  727 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3536346456012703\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.10343405 0.25170302 0.4221744  0.2226885 ]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  728 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.35383542112170746\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.07153514 0.2200878  0.50519264 0.20318441]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  731 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3537513961717551\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.07034208 0.21678866 0.5126385  0.20023078]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  732 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.3537235406972143\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.06962909 0.2149603  0.517477   0.1979336 ]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  737 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.3517787109273597\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.08587272 0.23167591 0.47466916 0.20778216]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  738 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.35175375101180617\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.06843536 0.21220571 0.52540535 0.19395351]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  739 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.35195408378070914\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.06800978 0.21136728 0.5282613  0.19236158]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  740 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.35215387584038427\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.06762498 0.21060438 0.53084767 0.19092296]] action proba 0.125 length 8 rew 1\n",
      "==========================================\n",
      "Episode:  741 length 8\n",
      "Reward:  0.125\n",
      "Mean Reward val 0.35184773854140805\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.0669063  0.20927888 0.53483325 0.18898165]] action proba 0.25 length 4 rew 1\n",
      "==========================================\n",
      "Episode:  743 length 4\n",
      "Reward:  0.25\n",
      "Mean Reward val 0.3512379327926408\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.06607308 0.2077353  0.5392584  0.18693319]] action proba 0.0 length 14 rew 0\n",
      "==========================================\n",
      "Episode:  744 length 14\n",
      "Reward:  0.0\n",
      "Mean Reward val 0.35076647248016746\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.06532164 0.20632783 0.5432763  0.18507421]] action proba 0.07142857142857142 length 14 rew 1\n",
      "==========================================\n",
      "Episode:  745 length 14\n",
      "Reward:  0.07142857142857142\n",
      "Mean Reward val 0.3503920248916264\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.06416054 0.20435856 0.54885375 0.18262714]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  747 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.35012359701758466\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.06362768 0.2029133  0.55275315 0.18070588]] action proba 0.25 length 4 rew 1\n",
      "==========================================\n",
      "Episode:  748 length 4\n",
      "Reward:  0.25\n",
      "Mean Reward val 0.34998992065307516\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.06324323 0.20209846 0.5540859  0.18057239]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  749 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3508566007588711\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.06202032 0.19944993 0.5596153  0.17891443]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  751 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3512532587355762\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.06101792 0.19713275 0.56385833 0.17799094]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  752 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3521148081927667\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.09859605 0.23790154 0.4570375  0.20646492]] action proba 0.25 length 4 rew 1\n",
      "==========================================\n",
      "Episode:  753 length 4\n",
      "Reward:  0.25\n",
      "Mean Reward val 0.35197937741267016\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.05819689 0.19123583 0.5764284  0.17413881]] action proba 0.1 length 10 rew 1\n",
      "==========================================\n",
      "Episode:  757 length 10\n",
      "Reward:  0.1\n",
      "Mean Reward val 0.3502538925714424\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.05731546 0.18963239 0.57987154 0.17318065]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  759 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3499900665383596\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.0563868  0.18780771 0.58375037 0.17205511]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  763 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.34946655833658813\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.05561106 0.1862049  0.5867865  0.17139754]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  764 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.35031692884856647\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.07789446 0.21521796 0.512054   0.19483356]] action proba 0.0 length 10 rew 0\n",
      "==========================================\n",
      "Episode:  766 length 10\n",
      "Reward:  0.0\n",
      "Mean Reward val 0.349403455761608\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.05577425 0.1854926  0.58163905 0.17709412]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  767 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.35025058667858505\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.05502163 0.18326625 0.5837457  0.17796646]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  768 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3510955143942176\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.05526577 0.18539254 0.5796744  0.17966726]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  769 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.35193824749240693\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.05469264 0.18546316 0.58020717 0.17963704]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  771 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.35232182716211574\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.05405384 0.18519315 0.5813836  0.17936948]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  772 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3531597031942475\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.05414419 0.18584426 0.5775218  0.18248974]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  777 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.3513184883065381\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.05429151 0.18676643 0.57356924 0.18537287]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  778 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.35150935032411634\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.05442173 0.18758564 0.5700118  0.18798086]] action proba 0.0 length 9 rew 0\n",
      "==========================================\n",
      "Episode:  779 length 9\n",
      "Reward:  0.0\n",
      "Mean Reward val 0.3510586973108804\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.054537   0.18831426 0.5668102  0.19033864]] action proba 0.2 length 5 rew 1\n",
      "==========================================\n",
      "Episode:  780 length 5\n",
      "Reward:  0.2\n",
      "Mean Reward val 0.3508652802848741\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.05449045 0.18868226 0.5648638  0.19196339]] action proba 0.2 length 5 rew 1\n",
      "==========================================\n",
      "Episode:  781 length 5\n",
      "Reward:  0.2\n",
      "Mean Reward val 0.35067235793156865\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.05446017 0.18928294 0.5617537  0.19450316]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  782 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3515016397222052\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.05496435 0.19043706 0.5547578  0.19984086]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  783 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3516910508960289\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.05602012 0.19254002 0.5446096  0.20683025]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  785 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3520684273568533\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.05677651 0.19395782 0.53638065 0.21288499]] action proba 0.0 length 23 rew 0\n",
      "==========================================\n",
      "Episode:  787 length 23\n",
      "Reward:  0.0\n",
      "Mean Reward val 0.3511748526681303\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.05744904 0.19518304 0.5290231  0.21834487]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  788 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.35136347769643433\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.05804767 0.19624494 0.52244335 0.223264  ]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  790 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.35089648196690265\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.08197188 0.22476745 0.45468894 0.23857173]] action proba 0.2 length 5 rew 1\n",
      "==========================================\n",
      "Episode:  791 length 5\n",
      "Reward:  0.2\n",
      "Mean Reward val 0.3507059561058333\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.05877635 0.19812055 0.51186055 0.23124252]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  792 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.3506840486370156\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.05911967 0.199281   0.50686777 0.2347315 ]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  793 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.35066219635073886\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.05903328 0.19908856 0.50436884 0.23750937]] action proba 0.25 length 4 rew 1\n",
      "==========================================\n",
      "Episode:  795 length 4\n",
      "Reward:  0.25\n",
      "Mean Reward val 0.3500952059076466\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.05896616 0.1989323  0.50156397 0.24053754]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  796 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3509106447960937\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.08325169 0.22646612 0.4399267  0.2503554 ]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  797 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3510974735620134\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.05630568 0.19564535 0.50535214 0.24269687]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  798 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3519096168992324\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.05396999 0.19265486 0.511385   0.24199013]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  799 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3520947298781084\n",
      "Max reward so far:  1.0\n",
      "Model saved\n",
      "0.645\n"
     ]
    }
   ],
   "source": [
    "# Parameters for the training\n",
    "max_episodes = 800\n",
    "gamma = 0.95 # Discount rate\n",
    "#max_batch = NbStat*5\n",
    "max_batch = 10*3\n",
    "episodes_succeded = 0\n",
    "\n",
    "client.run() # This run the main loop of ROS\n",
    "teresa_controller = Teresa(client) # Robot API\n",
    "env = RobotEnv(teresa_controller, client) # Training Environment\n",
    "\n",
    "env.reset() # Restarting the environment to the initial state\n",
    "write_op = tf.summary.merge_all()\n",
    "allRewards = []\n",
    "total_rewards = 0\n",
    "maximumRewardRecorded = 0\n",
    "episode = 0\n",
    "episode_states, episode_actions, episode_rewards = [],[],[]\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "NbStat = state_size\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    #writer = tf.summary.FileWriter(\"./tensorboard/pg/1\",sess.graph)\n",
    "     \n",
    "    for episode in range(max_episodes):\n",
    "        \n",
    "        episode_rewards_sum = 0\n",
    "\n",
    "        # Launch the game\n",
    "        state = env.reset()\n",
    "        print(NbStat)\n",
    "        ne_state=np.identity(NbStat)[state:state+1]\n",
    "        episode_length=0\n",
    "        while True:\n",
    "            episode_length+=1\n",
    "            if episode_length > max_batch:\n",
    "                print (\"tooooooooo long\")\n",
    "                break      \n",
    "            # Choose action a, remember WE'RE NOT IN A DETERMINISTIC ENVIRONMENT, WE'RE OUTPUT PROBABILITIES.\n",
    "            #state=int(state)\n",
    "            #print(\"state\",state,NbStat)\n",
    "            ne_state=np.identity(NbStat)[state:state+1]\n",
    "            action_probability_distribution = sess.run(action_distribution, feed_dict={input_: ne_state.reshape([1,NbStat])})\n",
    "            action = np.random.choice(range(action_probability_distribution.shape[1]), p=action_probability_distribution.ravel())  # select action w.r.t the actions prob\n",
    "\n",
    "            # Perform a\n",
    "            ext=False\n",
    "            #print(\"in actor mstep\",state,\"real ibn self\")\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "            env.render()\n",
    "            #print(\"after action state\",new_state,NbStat)\n",
    "            #print(\" drz actor mstep\",state,\"real ibn self\",new_state,\"rew\",reward,\"done\",done)\n",
    "            # Store s, a, r\n",
    "            episode_states.append(ne_state)\n",
    "                        \n",
    "            # For actions because we output only one (the index) we need 2 (1 is for the action taken)\n",
    "            # We need [0., 1.] (if we take right) not just the index\n",
    "            action_ = np.zeros((action_size), dtype=int)\n",
    "            action_[action] = 1\n",
    "            \n",
    "            #print(\"action proba\",action_probability_distribution,\"st\",state,\"new\",new_state)\n",
    "            state = new_state\n",
    "            episode_actions.append(action_)\n",
    "            \n",
    "            episode_rewards.append(reward)\n",
    "            if done:\n",
    "                # Calculate sum reward\n",
    "                if reward == 1:\n",
    "                    episodes_succeded += 1\n",
    "                \n",
    "                episode_rewards_sum = np.sum(episode_rewards)/episode_length  # HA addded the sum \n",
    "                print(action_probability_distribution,\"action proba\",episode_rewards_sum,\"length\",episode_length,\"rew\",reward)\n",
    "                allRewards.append(episode_rewards_sum)\n",
    "                \n",
    "                total_rewards = np.sum(allRewards)\n",
    "                \n",
    "                # Mean reward\n",
    "                mean_reward = np.divide(total_rewards, episode+1)\n",
    "                \n",
    "                \n",
    "                maximumRewardRecorded = np.amax(allRewards)\n",
    "                \n",
    "                print(\"==========================================\")\n",
    "                print(\"Episode: \", episode,\"length\",episode_length)\n",
    "                print(\"Reward: \", episode_rewards_sum)\n",
    "                print(\"Mean Reward\",\"val\",mean_reward)\n",
    "                print(\"Max reward so far: \", maximumRewardRecorded)\n",
    "                \n",
    "                # Calculate discounted reward\n",
    "                discounted_episode_rewards = discount_and_normalize_rewards(episode_rewards)\n",
    "                \n",
    "                #print(\"disco\",discounted_episode_rewards)               \n",
    "                # Feedforward, gradient and backpropagation\n",
    "                #loss_, _ = sess.run([loss, train_opt], feed_dict={input_: np.vstack(np.array(episode_states)),\n",
    "                #                                                 actions: np.vstack(np.array(episode_actions)),\n",
    "                #                                                 discounted_episode_rewards_: discounted_episode_rewards \n",
    "                #                                                })\n",
    "                loss_, _ = sess.run([loss, train_opt], feed_dict={input_: np.vstack(np.array(episode_states)),\n",
    "                                                                 actions: np.vstack(np.array(episode_actions)),\n",
    "                                                                 discounted_episode_rewards_: discounted_episode_rewards\n",
    "                                                                })\n",
    "                \n",
    " \n",
    "                                                                 \n",
    "                # Write TF Summaries\n",
    "                summary = sess.run(write_op, feed_dict={input_: np.vstack(np.array(episode_states)),actions: np.vstack(np.array(episode_actions)),discounted_episode_rewards_: discounted_episode_rewards,\n",
    "                                                                    mean_reward_: mean_reward  })\n",
    "                \n",
    "               \n",
    "#                 writer.add_summary(summary, episode)\n",
    "#                 writer.flush()\n",
    "                \n",
    "            \n",
    "                \n",
    "                # Reset the transition stores\n",
    "                episode_states, episode_actions, episode_rewards = [],[],[]\n",
    "                \n",
    "                break\n",
    "            \n",
    "            \n",
    "            \n",
    "        # Save Model\n",
    "    saver.save(sess, \"pgpendul.ckpt\")\n",
    "    print(\"Model saved\")\n",
    "    print(episodes_succeded / max_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "516\n",
      "2020-11-27 18:08:38,885     INFO: WebSocket connection closed: Code=1006, Reason=connection was closed uncleanly (peer dropped the TCP connection without previous WebSocket closing handshake)\n",
      "2020-11-27 18:08:38,889     INFO: <twisted.internet.tcp.Connector instance at 0x7fb1c5b7a8d0 disconnected IPv4Address(type='TCP', host='localhost', port=9090)> will retry in 2 seconds\n",
      "2020-11-27 18:08:38,892     INFO: Stopping factory <roslibpy.comm.comm_autobahn.AutobahnRosBridgeClientFactory object at 0x7fb254741940>\n",
      "2020-11-27 18:08:41,469     INFO: Starting factory <roslibpy.comm.comm_autobahn.AutobahnRosBridgeClientFactory object at 0x7fb254741940>\n",
      "2020-11-27 18:08:41,471     INFO: <twisted.internet.tcp.Connector instance at 0x7fb1c5b7a8d0 disconnected IPv4Address(type='TCP', host='localhost', port=9090)> will retry in 6 seconds\n",
      "2020-11-27 18:08:41,472     INFO: Stopping factory <roslibpy.comm.comm_autobahn.AutobahnRosBridgeClientFactory object at 0x7fb254741940>\n",
      "2020-11-27 18:08:47,746     INFO: Starting factory <roslibpy.comm.comm_autobahn.AutobahnRosBridgeClientFactory object at 0x7fb254741940>\n",
      "2020-11-27 18:08:47,750     INFO: <twisted.internet.tcp.Connector instance at 0x7fb1c5b7a8d0 disconnected IPv4Address(type='TCP', host='localhost', port=9090)> will retry in 20 seconds\n",
      "2020-11-27 18:08:47,750     INFO: Stopping factory <roslibpy.comm.comm_autobahn.AutobahnRosBridgeClientFactory object at 0x7fb254741940>\n"
     ]
    }
   ],
   "source": [
    "print(episodes_succeded)"
   ]
  },
  {
   "source": [
    "## Testing the Neural Network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Structure of the NN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "NbStat = NB_STATES\n",
    "state_size = NbStat\n",
    "action_size = 4\n",
    "# new_graph = tf.Graph()\n",
    "initializer=tf.initializers.glorot_uniform()\n",
    "learning_rate = 0.01\n",
    "\n",
    "def discount_correct_rewards(r, gamma=0.99):\n",
    "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "  discounted_r = np.zeros_like(r)\n",
    "  running_add = 0\n",
    "  for t in reversed(range(0, r.size)):\n",
    "    #if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
    "    running_add = running_add * gamma + r[t]\n",
    "    discounted_r[t] = running_add\n",
    "\n",
    "  discounted_r -= discounted_r.mean()\n",
    "  discounted_r /- discounted_r.std()\n",
    "  return discounted_r\n",
    "\n",
    "def discount_and_normalize_rewards(episode_rewards):\n",
    "    discounted_episode_rewards = np.zeros_like(episode_rewards)\n",
    "    cumulative = 0.0\n",
    "    #print(\"len episode rewards\",episode_rewards)\n",
    "    for i in reversed(range(len(episode_rewards))):\n",
    "        cumulative = cumulative * gamma + episode_rewards[i]\n",
    "        #print(\"dans boucle\",episode_rewards[i],\"cyl\",cumulative)\n",
    "        discounted_episode_rewards[i] = cumulative\n",
    "    \n",
    "    mean = np.mean(discounted_episode_rewards)\n",
    "    std = np.std(discounted_episode_rewards)\n",
    "    if std :\n",
    "        discounted_episode_rewards = (discounted_episode_rewards - mean) / (std)\n",
    "    else:\n",
    "        discounted_episode=[]\n",
    "        discounted_episode_rewards[0] = np.array(mean)\n",
    "        print(\"ATTTTTTTTTTTTTTTTTT\")\n",
    "    #print(\"dis\",discounted_episode_rewards,\"std\",std)\n",
    "    \n",
    "    return discounted_episode_rewards\n",
    "\n",
    "with tf.name_scope(\"inputs\"):\n",
    "    input_ = tf.placeholder(tf.float32, [None, state_size], name=\"input_\")\n",
    "    actions = tf.placeholder(tf.int32, [None, action_size], name=\"actions\")\n",
    "    discounted_episode_rewards_ = tf.placeholder(tf.float32, [None,], name=\"discounted_episode_rewards\")\n",
    "    \n",
    "    # Add this placeholder for having this variable in tensorboard\n",
    "    mean_reward_ = tf.placeholder(tf.float32 , name=\"mean_reward\")\n",
    "\n",
    "    with tf.name_scope(\"fc1\"):\n",
    "        fc1 = tf.layers.dense(input_ , 20, activation=tf.nn.relu,kernel_initializer=initializer)\n",
    "\n",
    "    with tf.name_scope(\"fc2\"):\n",
    "        fc2 = tf.layers.dense(fc1, action_size,activation= tf.nn.relu, kernel_initializer=initializer)\n",
    "    \n",
    "    with tf.name_scope(\"fc3\"):\n",
    "        fc3 = tf.layers.dense(fc2, action_size, activation= None,kernel_initializer=initializer)\n",
    "\n",
    "    with tf.name_scope(\"softmax\"):\n",
    "        action_distribution = tf.nn.softmax(fc3)\n",
    "\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        # tf.nn.softmax_cross_entropy_with_logits computes the cross entropy of the result after applying the softmax function\n",
    "        # If you have single-class labels, where an object can only belong to one class, you might now consider using \n",
    "        # tf.nn.sparse_softmax_cross_entropy_with_logits so that you don't have to convert your labels to a dense one-hot array. \n",
    "        neg_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(logits = fc3, labels = actions)\n",
    "        #loss = tf.nn.sparse_softmax_cross_entropy_with_logits (neg_log_prob * discounted_episode_rewards_)\n",
    "        loss = tf.reduce_mean(neg_log_prob * discounted_episode_rewards_) \n",
    "        \n",
    "    \n",
    "    with tf.name_scope(\"train\"):\n",
    "        train_opt = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "# Setup TensorBoard Writer\n",
    "\n",
    "\n",
    "## Losses\n",
    "## TRAINING Hyperparameters\n",
    "\n",
    "# tf.summary.scalar(\"Loss\", loss)\n",
    "\n",
    "# ## Reward mean\n",
    "# tf.summary.scalar(\"Reward_mean\", mean_reward_)\n",
    "\n",
    "max_episodes = 500\n",
    "\n",
    "gamma = 0.95 # Discount rate\n",
    "max_batch = NbStat*5\n",
    "    \n",
    "episode_rewards_sum = 0\n",
    "\n",
    "        # Launch the game\n",
    "    #state = env.reset()\n",
    "    #ne_state=np.identity(NbStat)[state:state+1]\n",
    "    #env.render()\n",
    "episode_length=0"
   ]
  },
  {
   "source": [
    "### Running the NN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "client.run() # This run the main loop of ROS\n",
    "teresa_controller = Teresa(client) # Robot API\n",
    "env = RobotEnv(teresa_controller, client) # Training Environment\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # sess.run(tf.global_variables_initializer())\n",
    "   \n",
    "        # Load the model\n",
    "    print(saver.restore(sess, \"pgpendul.ckpt\"))\n",
    "    # if not saver.restore(sess, \"pgpendul.ckpt\"):\n",
    "    #     print()\n",
    "    total_rewards = 0\n",
    "    for episode in range(50):\n",
    "        state = env.reset()\n",
    "        #ne_state=np.identity(NbStat)[state:state+1]\n",
    "        step = 0\n",
    "        done = False\n",
    "        \n",
    "        print(\"****************************************************\")\n",
    "        print(\"EPISODE \", episode)\n",
    "\n",
    "       \n",
    "        #while True:\n",
    "        j = 0\n",
    "        #The Q-Network\n",
    "        while j < 500:\n",
    "            j+=1\n",
    "            state=int(state)\n",
    "            ne_state=np.identity(NbStat)[state:state+1]\n",
    "            # Choose action a, remember WE'RE NOT IN A DETERMINISTIC ENVIRONMENT, WE'RE OUTPUT PROBABILITIES.\n",
    "            action_probability_distribution = sess.run(action_distribution, feed_dict={input_: ne_state.reshape([1,NbStat])})\n",
    "            print(action_probability_distribution)\n",
    "            action = np.random.choice(range(action_probability_distribution.shape[1]), p=action_probability_distribution.ravel())  # select action w.r.t the actions prob\n",
    "            #action = np.argmax(action_probability_distribution)\n",
    "            \n",
    "\n",
    "            # new_state, reward, done, info = env.step(int(action),True)\n",
    "            new_state, reward, done, info = env.step(int(action))\n",
    "\n",
    "            print(\"state\",state,\"ne_state\",new_state,\"action\",action) \n",
    "            total_rewards += reward\n",
    "            env.render()\n",
    "            if done:    \n",
    "                #rewards.append(total_rewards)\n",
    "                print (\"Score\", total_rewards)\n",
    "                break\n",
    "            state = new_state\n",
    "    env.close()\n",
    "print (\"Score over time: \" ,  total_rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('venv': virtualenv)",
   "language": "python",
   "name": "python36964bitvenvvirtualenv7e3520635ff84bc18d17652efd63c9bd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}